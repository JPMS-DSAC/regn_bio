{
 "cells": [
  {
   "source": [
    "Biography Analysis - Generates the required files for Amendment Biography"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two pieces of text, there are 3 modules running\n",
    "\n",
    "Mod 1 : Compare the two pieces to find exact matches\n",
    "Mod 2 : Find differences given lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "# import itertools\n",
    "from itertools import product\n",
    "from difflib import SequenceMatcher\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_para(text):\n",
    "    \"\"\"\n",
    "    Split a paragraph (a regulation) into sentences\n",
    "    \"\"\"\n",
    "    return nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import ratio\n",
    "def naive_string_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Get levenshtein string similarity\n",
    "    \"\"\"\n",
    "    if type(a) == list and type(b) == list:\n",
    "        a = \" \".join(a)\n",
    "        b = \" \".join(b)\n",
    "        \n",
    "    return ratio(a,b)\n",
    "    \n",
    "def full_score(sent_pair,s1,s2,threshold=0.9):\n",
    "    \"\"\"\n",
    "    Check if a string is perfectly matching\n",
    "    \"\"\"\n",
    "    i1 , i2 = sent_pair\n",
    "#     print(s1[i1],s2[i2])\n",
    "#     print(\"~~~~~~~~~~~~~~~\")\n",
    "    if type(s1[i1]) is list and type(s2[i2]) is list:\n",
    "        l1 = s1[i1][1]\n",
    "        l2 = s2[i2][1]\n",
    "    elif type(s1[i1]) is str and type(s2[i2]) is str:\n",
    "        l1 = s1[i1]\n",
    "        l2 = s2[i2]\n",
    "    return True if naive_string_similarity(l1,l2) >= threshold else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fully_matching_sent_indices(s1,s2):\n",
    "    \"\"\"Assumes that the no there are no repetitions in the given list, causing (i1,i2),(i3,i2) and s1[i1] == s1[i3]\n",
    "    Returns all pairs that match fully (however full score is defined) , \n",
    "    in sorted order of the second tuple-element \"\"\"\n",
    "    i1 = range(len(s1))\n",
    "    i2 = range(len(s2))\n",
    "    \n",
    "    all_combos = list(product(i1,i2))\n",
    "    full_matches = [sent_pair for sent_pair in all_combos if full_score(sent_pair,s1,s2)]\n",
    "    \n",
    "    return sorted(full_matches, key=lambda x : x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def get_maximum_matching(s1,s2):\n",
    "    \"\"\"\n",
    "    s1 = List of [reg_num, list of sub regs]\n",
    "    Returns min(s1,s2) matching tuples\n",
    "    \"\"\"\n",
    "    i1 = range(len(s1))\n",
    "    i2 = range(len(s2))\n",
    "    \n",
    "    set_i1 = set(range(len(s1)))\n",
    "    set_i2 = set(range(len(s2)))\n",
    "    \n",
    "    all_combos = list(product(i1,i2))\n",
    "    \n",
    "    scores = [(-naive_string_similarity(s1[ind1],s2[ind2]),ind1,ind2) for ind1,ind2 in all_combos]\n",
    "    heapq.heapify(scores)\n",
    "    score_threshold = 0.8\n",
    "    \"\"\"\n",
    "    Even if there is a match it is useless after 0.8\n",
    "    \"\"\"\n",
    "    max_tups = min(len(i1),len(i2))\n",
    "    count = 0\n",
    "    candidate_tuples = []\n",
    "    \n",
    "    matched_i1 = set()\n",
    "    matched_i2 = set()\n",
    "    while scores != []:\n",
    "        score, ind1, ind2 = heapq.heappop(scores)\n",
    "        if -score >= score_threshold and ind1 in set_i1 and ind2 in set_i2:\n",
    "            candidate_tuples.append((-score,ind1,ind2))\n",
    "            set_i1.remove(ind1)\n",
    "            set_i2.remove(ind2)\n",
    "            \n",
    "            matched_i1.add(ind1)\n",
    "            matched_i2.add(ind2)\n",
    "    \n",
    "    return candidate_tuples, matched_i1, matched_i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRED = '\\033[91m'\n",
    "CEND = '\\033[0m'\n",
    "CBLUE = '\\033[34m'\n",
    "CGREEN = '\\033[92m'\n",
    "\n",
    "printr = lambda inp : CRED + inp + CEND\n",
    "printb = lambda inp : CBLUE + inp + CEND\n",
    "printg = lambda inp : CGREEN + inp + CEND\n",
    "\n",
    "print_join = lambda inp : print(\" \".join(inp))\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def lcs(a, b):\n",
    "    \"\"\"\n",
    "    Word level LCS\n",
    "    a : array of words\n",
    "    b : array of words\n",
    "    \"\"\"\n",
    "    # generate matrix of length of longest common subsequence for substrings of both words\n",
    "    lengths = [[0] * (len(b)+1) for _ in range(len(a)+1)]\n",
    "    for i, x in enumerate(a):\n",
    "        for j, y in enumerate(b):\n",
    "            if x == y:\n",
    "                lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "            else:\n",
    "                lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    " \n",
    "    # read a substring from the matrix\n",
    "    \n",
    "    result = []\n",
    "    j = len(b)\n",
    "    for i in range(1, len(a)+1):\n",
    "        if lengths[i][j] != lengths[i-1][j]:\n",
    "            result.append(a[i-1])\n",
    "    \n",
    "    qa = deque(a)\n",
    "    qb = deque(b)\n",
    "    rq = deque(result)\n",
    "    \n",
    "    \n",
    "    ansa = []\n",
    "    ansb = []\n",
    "    \n",
    "    while rq :\n",
    "        common_ele = rq.popleft()\n",
    "        while len(qa) >= 1 and qa[0] != common_ele:\n",
    "            ansa.append(printb(qa.popleft()))\n",
    "        while len(qb) >= 1 and qb[0] != common_ele:\n",
    "            ansb.append(printb(qb.popleft()))\n",
    "        \n",
    "        ansa.append(printr(common_ele))\n",
    "        ansb.append(printr(common_ele))\n",
    "        \n",
    "        if len(qa) >= 1:\n",
    "            qa.popleft()\n",
    "        if len(qb) >= 1:\n",
    "            qb.popleft()\n",
    "    \n",
    "    while len(qa) >= 1:\n",
    "        ansa.append(printb(qa.popleft()))\n",
    "    while len(qb) >= 1:\n",
    "        ansb.append(printb(qb.popleft()))\n",
    "    \n",
    "    return result, ansa, ansb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize as wdtk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_two_regs(s1,s2,debug=False):\n",
    "    \"\"\"\n",
    "    If the sorted array is increasing in both components, done ... problem solved !!\n",
    "    TODO : Handle when the rows are jumbled\n",
    "    match_lines : gives an array of tuples, x[i] belongs to index(si+1)\n",
    "    non_match_areas : gives an array of \"list of list of size (2,2)\", \n",
    "                      format is [[start_index_s1, end_index_s1],[start_index_s2, end_index_s2]]\n",
    "    \n",
    "    \"\"\"\n",
    "    # check is to check both lists are in ascending order\n",
    "    check = True\n",
    "    \n",
    "    full_matches = []\n",
    "    partial_matches = []\n",
    "    no_matches = [[],[]]\n",
    "    \n",
    "    full_match_pairs = get_fully_matching_sent_indices(s1,s2,threshold=0.9)\n",
    "    \n",
    "    #edges_added_to_full_pairs = [(0,0)] + full_match_pairs + [len(s1),len(s2)]\n",
    "    #in the form p1 start:end, p2 start:end\n",
    "    if len(full_match_pairs) > 0:\n",
    "        non_match_areas = [ [[0,full_match_pairs[0][0]],[0,full_match_pairs[0][1]]] ]\n",
    "    \n",
    "    if check:\n",
    "        for i, elem in enumerate(full_match_pairs):\n",
    "            if i == len(full_match_pairs) - 1:\n",
    "                non_match_areas.append([[elem[0]+1,len(s1)], [elem[1]+1, len(s2)] ])\n",
    "            else:\n",
    "                p11 , p12 = full_match_pairs[i]\n",
    "                p21,  p22 = full_match_pairs[i+1]\n",
    "                non_match_areas.append([ [p11+1, p21] , [p12+1, p22] ])\n",
    "    \n",
    "    if debug:\n",
    "        d = difflib.Differ()\n",
    "        for ele in full_match_pairs:\n",
    "            diff = d.compare([s1[ele[0]]], [s2[ele[1]]])\n",
    "            a,b,c = lcs(wdtk(s1[ele[0]]), wdtk(s2[ele[1]]))\n",
    "            \n",
    "            full_matches.append(\" \".join(a))\n",
    "            \n",
    "#             print_join(a)\n",
    "#             print_join(b)\n",
    "#             print_join(c)\n",
    "            \n",
    "    \n",
    "    if debug:\n",
    "        pass\n",
    "        #print(\"COMMON STRINGS DONE COMMON STRINGS DONE COMMON STRINGS DONE COMMON STRINGS DONE COMMON STRINGS DONE\")   \n",
    "    \n",
    "    if debug:\n",
    "        for elem in non_match_areas:\n",
    "            p11, p12 = elem[0]\n",
    "            p21, p22 = elem[1]\n",
    "            if p11 == p12 and p21 == p22:\n",
    "                continue\n",
    "            l1 = s1[p11:p12]\n",
    "            l2 = s2[p21:p22]\n",
    "            \n",
    "            max_match_tuples = get_maximum_matching(l1,l2)\n",
    "            \n",
    "            all_combos , mi1, mi2  = get_maximum_matching(l1,l2)\n",
    "            \n",
    "            for a,b,c in all_combos : \n",
    "#                 print(\"MATCHED --\")\n",
    "#                 print(f\"SCORE : {a}\")\n",
    "                \n",
    "                a1,a2,a3 =  lcs(wdtk(l1[b]),wdtk(l2[c]))\n",
    "                partial_matches.append([\" \".join(a1),\" \".join(a2),\" \".join(a3)])\n",
    "#                 print_join(a1)\n",
    "#                 print_join(a2)\n",
    "#                 print_join(a3)\n",
    "                \n",
    "            \n",
    "            for ind, ele in enumerate(l1): \n",
    "                if ind not in mi1:\n",
    "                    no_matches[0].append(ele)\n",
    "#                     print(printg(ele), end='--\\n')\n",
    "#             print(printg('000000000000000000000000000000000000000000000000000000000000'))\n",
    "            \n",
    "            for ind, ele in enumerate(l2): \n",
    "                if ind not in mi2:\n",
    "                    no_matches[1].append(ele)\n",
    "#                     print(printg(ele), end='--\\n')\n",
    "    \n",
    "    return full_matches, partial_matches, no_matches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 = \"\"\"3.\\n(1) No insider shall communicate, provide, or allow access to any unpublished price sensitive information, relating to a company or securities listed or proposed to be listed, to any person including other insiders except where such communication is in furtherance of legitimate purposes, performance of duties or discharge of legal obligations.\\nNOTE\\n: \\nThis provision is intended to cast an obligation on all insiders who are essentially persons in possession of unpublished price sensitive information to handle such information with care and to deal with the information with them when transacting their business strictly on a \\nneed-to-know\\n basis. It is also intended to lead to organisations developing practices based on \\nneed-to-know\\n principles for treatment of information in their possession.\\n(2)\\nNo person shall procure from or cause the communication by any insider of unpublished price sensitive information, relating to a company or securities listed or proposed to be listed, except in furtherance of legitimate purposes, performance of duties or discharge of legal obligations.\\nNOTE\\n: This provision is intended to impose a prohibition on unlawfully procuring possession of unpublished price sensitive information. Inducement and procurement of unpublished price sensitive information not in furtherance of one’s legitimate duties and discharge of obligations would be illegal under this provision.\\n(3)\\nNotwithstanding anything contained in this regulation, an unpublished price sensitive information may be communicated, provided, allowed access to or procured, in connection with a transaction that would:–\\n(i)\\nentail an obligation to make an open offer under the takeover regulations where the board of directors of the company is of informed opinion that the proposed transaction is in the best interests of the company;\\nNOTE\\n: It is intended to acknowledge the necessity of communicating, providing, allowing access to or procuring UPSI for substantial transactions such as takeovers, mergers and acquisitions involving trading in securities and change of control to assess a potential investment. In an open offer under the takeover regulations, not only would the same price be made available to all shareholders of the company but also all information necessary to enable an informed divestment or retention decision by the public shareholders is required to be made available to all shareholders in the letter of offer under those regulations.\\n(ii)\\nnot attract the obligation to make an open offer under the takeover regulations but where the board of directors of the company is of informed opinion that the proposed transaction is in the best interests of the company and the information that constitute unpublished price sensitive information is disseminated to be made generally available at least two trading days prior to the proposed transaction being effected in such form as the board of directors may determine.\\nNOTE\\n: It is intended to permit communicating, providing, allowing access to or procuring UPSI also in transactions that do not entail an open offer obligation under the takeover regulations if it is in the best interests of the company. The board of directors, however, would cause public disclosures of such unpublished price sensitive information well before the proposed transaction to rule out any information asymmetry in the market.\\n(4)\\nFor purposes of \\nsub-regulation\\n (3), the board of directors shall require the parties to execute agreements to contract confidentiality and \\nnon-disclosure\\n obligations on the part of such parties and such parties shall keep information so received confidential, except for the purpose of \\nsub-regulation\\n\"\"\"\n",
    "# a2 = \"\"\"3.\\n(1) No insider shall communicate, provide, or allow access to any unpublished price sensitive information, relating to a company or securities listed or proposed to be listed, to any person including other insiders except where such communication is in furtherance of legitimate purposes, performance of duties or discharge of legal obligations.\\nNOTE\\n: \\nThis provision is intended to cast an obligation on all insiders who are essentially persons in possession of unpublished price sensitive information to handle such information with care and to deal with the information with them when transacting their business strictly on a \\nneed-to-know\\n basis. It is also intended to lead to organisations developing practices based on \\nneed-to-know\\n principles for treatment of information in their possession.\\n(2)\\nNo person shall procure from or cause the communication by any insider of unpublished price sensitive information, relating to a company or securities listed or proposed to be listed, except in furtherance of legitimate purposes, performance of duties or discharge of legal obligations.\\nNOTE\\n: This provision is intended to impose a prohibition on unlawfully procuring possession of unpublished price sensitive information. Inducement and procurement of unpublished price sensitive information not in furtherance of one’s legitimate duties and discharge of obligations would be illegal under this provision.\\n7\\n[(2A) The board of directors of a listed company shall make a policy for determination of “legitimate purposes” as a part of “Codes of Fair Disclosure and Conduct” formulated under regulation 8.\\nExplanation – For the purpose of illustration, the term “legitimate purpose” shall include sharing of unpublished price sensitive information in the ordinary course of business by an insider with\\n7\\nInserted by Securities and Exchange Board of India (Prohibition of Insider Trading) (Amendment) Regulations, 2018 (w.e.f. April 01, 2019).\\npartners, collaborators, lenders, customers, suppliers, merchant bankers, legal advisors, auditors, insolvency professionals or other advisors or consultants, provided that such sharing has not been carried out to evade or circumvent the prohibitions of these regulations.]\\n8\\n[(2B) Any person in receipt of unpublished price sensitive information pursuant to a “legitimate purpose” shall be considered an “insider” for purposes of these regulations and due notice shall be given to such persons to maintain confidentiality of such unpublished price sensitive information in compliance with these regulations.]\\n(3)\\nNotwithstanding anything contained in this regulation, an unpublished price sensitive information may be communicated, provided, allowed access to or procured, in connection with a transaction that would:–\\n(i)\\nentail an obligation to make an open offer under the takeover regulations where the board of directors of the \\n9\\n[listed] company is of informed opinion that \\n10\\n[sharing of such information] is in the best interests of the company;\\nNOTE\\n: It is intended to acknowledge the necessity of communicating, providing, allowing access to or procuring UPSI for substantial transactions such as takeovers, mergers and acquisitions involving trading in securities and change of control to assess a potential investment. In an open offer under the takeover regulations, not only would the same price be made available to all shareholders of the company but also all information necessary to enable an informed divestment or retention decision by the public shareholders is required to be made available to all shareholders in the letter of offer under those regulations.\\n8\\nInserted by Securities and Exchange Board of India (Prohibition of Insider Trading) (Amendment) Regulations, 2018 (w.e.f. April 01, 2019)\\n9\\nInserted by Securities and Exchange Board of India (Prohibition of Insider Trading) (Amendment) Regulations, 2018 (w.e.f. April 01, 2019)\\n10\\nSubstituted for the words “the proposed transaction” by Securities and Exchange Board of India (Prohibition of Insider Trading) (Amendment) Regulations, 2018 (w.e.f. April 01, 2019).\\n(ii)\\nnot attract the obligation to make an open offer under the takeover regulations but where the board of directors of the \\n11\\n[listed] company is of informed opinion \\n12\\n[that sharing of such information] is in the best interests of the company and the information that constitute unpublished price sensitive information is disseminated to be made generally available at least two trading days prior to the proposed transaction being effected in such form as the board of directors may determine \\n13\\n[to be adequate and fair to cover all relevant and material facts].\\nNOTE\\n: It is intended to permit communicating, providing, allowing access to or procuring UPSI also in transactions that do not entail an open offer obligation under the takeover regulations \\n14\\n[when authorised by the board of directors if sharing of such information] is in the best interests of the company. The board of directors, however, would cause public disclosures of such unpublished price sensitive information well before the proposed transaction to rule out any information asymmetry in the market.\\n(4)\\nFor purposes of \\nsub-regulation\\n (3), the board of directors shall require the parties to execute agreements to contract confidentiality and \\nnon-disclosure\\n obligations on the part of such parties and such parties shall keep information so received confidential, except for the purpose of \\nsub-regulation\\n (3), and shall not otherwise trade in securities of the company when in possession of unpublished price sensitive information.\\n15\\n[(5) The board of directors or head(s) of the organisation of every person required to handle unpublished price sensitive information shall ensure that a structured digital database is\\n11\\nInserted by Securities and Exchange Board of India (Prohibition of Insider Trading) (Amendment) Regulations, 2018 (w.e.f. April 01, 2019)\\n12\\nSubstituted for the words “that the proposed transaction” by Securities and Exchange Board of India (Prohibition of Insider Trading) (Amendment) Regulations, 2018 (w.e.f. April 01, 2019)\\n13\\nInserted by Securities and Exchange Board of India (Prohibition of Insider Trading) (Amendment) Regulations, 2018 (w.e.f. April 01, 2019)\\n14\\nSubstituted for the words “if it” by Securities and Exchange Board of India (Prohibition of Insider Trading) (Amendment) Regulations, 2018 (w.e.f. April 01, 2019)\\n15\\nSubstituted by Securities and Exchange Board of India (Prohibition of Insider Trading) (Amendment) Regulations, 2020 (w.e.f. July 17, 2020). Prior to the substitution, \\nsub-regulation\\n 5 read as follows: -\\n“The board of directors shall ensure that a structured digital database is maintained containing the names of such persons or entities as the case may be with whom information is shared under this regulation along with the Permanent Account Number or any other identifier authorized by law where Permanent Account\\nmaintained containing the nature of unpublished price sensitive information and the names of such persons who havThe  board  of  directors or  head(s)  of  the  organisation  of  every  person  required  to  handle e shared the information and also the names of such persons with whom information is shared under this regulation along with the Permanent Account Number or any other identifThe  board  of  directors or  head(s)  of  the  organisation  of  every  person  required  to  handle ier authorized by law where Permanent Account Number is not available. Such database shall not be outsourced and shall be maintained internally with adequate internal controls and checks such as time stamping and audit trails to ensure \\nnon-tampering\\n of the database.]\\n16\\n\"\"\"\n",
    "\n",
    "# s1 = [ele.replace('\\n','~~') for ele in split_para(a1) if len(ele) > 10 and 'Inserted' not in ele and 'Substituted' not in ele and 'NOTE' not in ele]\n",
    "# s2 = [ele.replace('\\n','~~') for ele in split_para(a2) if len(ele) > 10 and 'Inserted' not in ele and 'Substituted' not in ele and 'NOTE' not in ele]\n",
    "\n",
    "# len(s1),len(s2)\n",
    "# f,p,n = match_two_regs(s1,s2,debug=True)\n",
    "\n",
    "# f\n",
    "# for ele in p:\n",
    "#     print(ele[0])\n",
    "#     print(ele[1])\n",
    "#     print(ele[2])\n",
    "# print(n[0])\n",
    "# print(n[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Script to compare two docs and get most similar docs, \n",
    "Assumes document is an array of regulations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_list_to_linear_list(list_):\n",
    "    ans = []\n",
    "    for ele in list_:\n",
    "        if type(ele) is str:\n",
    "            ans.append(ele)\n",
    "        else:\n",
    "            ans.append(''.join(ele))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_regs(s1,s2):\n",
    "    \"\"\"\n",
    "    Returns min(s1,s2) matching tuples\n",
    "    \"\"\"\n",
    "    i1 = range(len(s1))\n",
    "    i2 = range(len(s2))\n",
    "    \n",
    "    set_i1 = set(range(len(s1)))\n",
    "    set_i2 = set(range(len(s2)))\n",
    "    \n",
    "    all_combos = list(product(i1,i2))\n",
    "    scores = [(-naive_string_similarity(s1[ind1][1],s2[ind2][1]),ind1,ind2) for ind1,ind2 in all_combos]\n",
    "    heapq.heapify(scores)\n",
    "    score_threshold = 0.0\n",
    "    \"\"\"\n",
    "    Even if there is a match it is useless after score_threshold\n",
    "    \"\"\"\n",
    "    max_tups = min(len(i1),len(i2))\n",
    "    count = 0\n",
    "    candidate_tuples = []\n",
    "    \n",
    "    matched_i1 = set()\n",
    "    matched_i2 = set()\n",
    "    while scores != []:\n",
    "        score, ind1, ind2 = heapq.heappop(scores)\n",
    "        if -score >= score_threshold and ind1 in set_i1 and ind2 in set_i2:\n",
    "            candidate_tuples.append((-score,ind1,ind2))\n",
    "            set_i1.remove(ind1)\n",
    "            set_i2.remove(ind2)\n",
    "            \n",
    "            matched_i1.add(ind1)\n",
    "            matched_i2.add(ind2)\n",
    "    \n",
    "    return candidate_tuples, matched_i1, matched_i2, set_i1, set_i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a,b,c,d,e = get_matching_regs(list_d1, list_d2)\n",
    "\n",
    "# print(len(list_d1),len(list_d2))\n",
    "\n",
    "# for ele in a:\n",
    "#     i1, i2, i3 = ele\n",
    "#     print(f\"SCORE {i1}\")\n",
    "#     print(printr(list_d1[i2]))\n",
    "#     print(printb(list_d2[i3]))\n",
    "#     print(\"------------------------\")\n",
    "\n",
    "# print(f\"LEFT OVERS\")\n",
    "# for ele in d:\n",
    "#     print(printr(list_d1[ele]))\n",
    "#     print(\"------------------------\")\n",
    "\n",
    "# for ele in e:\n",
    "#     print(printr(list_d1[ele]))\n",
    "#     print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Generate a HTML table given two documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs_for_html(a, b):\n",
    "    \"\"\"\n",
    "    Word level LCS, with colors\n",
    "    a : array of words\n",
    "    b : array of words\n",
    "    \"\"\"\n",
    "    # generate matrix of length of longest common subsequence for substrings of both words\n",
    "    lengths = [[0] * (len(b)+1) for _ in range(len(a)+1)]\n",
    "    for i, x in enumerate(a):\n",
    "        for j, y in enumerate(b):\n",
    "            if x == y:\n",
    "                lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "            else:\n",
    "                lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "    \n",
    "    result = []\n",
    "    j = len(b)\n",
    "    for i in range(1, len(a)+1):\n",
    "        if lengths[i][j] != lengths[i-1][j]:\n",
    "            result.append(a[i-1])\n",
    "    \n",
    "    qa = deque(a)\n",
    "    qb = deque(b)\n",
    "    rq = deque(result)\n",
    "    \n",
    "    \n",
    "    ansa = []\n",
    "    ansb = []\n",
    "    \n",
    "    color_a = []\n",
    "    color_b = []\n",
    "    \n",
    "    while rq :\n",
    "        common_ele = rq.popleft()\n",
    "        while len(qa) >= 1 and qa[0] != common_ele:\n",
    "            ansa.append(qa.popleft())\n",
    "            color_a.append(1)\n",
    "        while len(qb) >= 1 and qb[0] != common_ele:\n",
    "            ansb.append(qb.popleft())\n",
    "            color_b.append(1)\n",
    "        \n",
    "        ansa.append(common_ele)\n",
    "        ansb.append(common_ele)\n",
    "        color_a.append(0)\n",
    "        color_b.append(0)\n",
    "        \n",
    "        if len(qa) >= 1:\n",
    "            qa.popleft()\n",
    "        if len(qb) >= 1:\n",
    "            qb.popleft()\n",
    "    \n",
    "    while len(qa) >= 1:\n",
    "        ansa.append(qa.popleft())\n",
    "        color_a.append(1)\n",
    "        \n",
    "    while len(qb) >= 1:\n",
    "        ansb.append(qb.popleft())\n",
    "        color_b.append(1)\n",
    "    \n",
    "    return result, ansa, ansb, color_a, color_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_two_regs_for_html(tup_s1,tup_s2,debug=False):\n",
    "    \"\"\"\n",
    "    If the sorted array is increasing in both components, done ... problem solved !!\n",
    "    TODO : Handle when the rows are jumbled\n",
    "    match_lines : gives an array of tuples, x[i] belongs to index(si+1)\n",
    "    non_match_areas : gives an array of \"list of list of size (2,2)\", \n",
    "                      format is [[start_index_s1, end_index_s1],[start_index_s2, end_index_s2]]\n",
    "    \n",
    "    \"\"\"\n",
    "    check = True\n",
    "    s1 = tup_s1\n",
    "    s2 = tup_s2\n",
    "    \n",
    "    full_matches = []\n",
    "    partial_matches = []\n",
    "    no_matches = [[],[]]\n",
    "    \n",
    "    full_match_pairs = get_fully_matching_sent_indices(s1,s2)\n",
    "    non_match_areas = []\n",
    "    \n",
    "    if len(full_match_pairs) > 0:\n",
    "        non_match_areas = [ [[0,full_match_pairs[0][0]],[0,full_match_pairs[0][1]]] ]\n",
    "    \n",
    "    for i, elem in enumerate(full_match_pairs):\n",
    "        if i == len(full_match_pairs) - 1:\n",
    "            non_match_areas.append([[elem[0]+1,len(s1)], [elem[1]+1, len(s2)] ])\n",
    "        else:\n",
    "            p11 , p12 = full_match_pairs[i]\n",
    "            p21,  p22 = full_match_pairs[i+1]\n",
    "            non_match_areas.append([ [p11+1, p21] , [p12+1, p22] ])\n",
    "   \n",
    "    \n",
    "    d = difflib.Differ()\n",
    "    for ele in full_match_pairs:\n",
    "        full_matches.append([ele[0],ele[1]])\n",
    "    \n",
    "    if len(full_match_pairs) == 0:\n",
    "        non_match_areas.append([[0,len(s1)],[0,len(s2)]])\n",
    "    \n",
    "    for elem in non_match_areas:\n",
    "        p11, p12 = elem[0]\n",
    "        p21, p22 = elem[1]\n",
    "        if p11 == p12 and p21 == p22:\n",
    "            continue\n",
    "        l1 = s1[p11:p12]\n",
    "        l2 = s2[p21:p22]\n",
    "\n",
    "        all_combos , mi1, mi2  = get_maximum_matching(l1,l2)\n",
    "\n",
    "        for a,b,c in all_combos : \n",
    "            a1,a2,a3,c1,c2 =  lcs_for_html(wdtk(l1[b]),wdtk(l2[c]))\n",
    "            partial_matches.append([a2,a3,p11 + b, p21 + c, c1 , c2])\n",
    "\n",
    "        for ind, ele in enumerate(l1): \n",
    "            if ind not in mi1:\n",
    "                no_matches[0].append(ind + p11)\n",
    "\n",
    "        for ind, ele in enumerate(l2): \n",
    "            if ind not in mi2:\n",
    "                no_matches[1].append(ind + p21)\n",
    "                    \n",
    "    return full_matches, partial_matches, no_matches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_html(doc1, doc2, doc_names=None):\n",
    "    if doc_names is None:\n",
    "        doc_names = [\"Document 1\", \"Document 2\"]\n",
    "    \n",
    "    html_obj = f\"\"\"\n",
    "                <table width='100%'>\n",
    "                <tr>\n",
    "                <th>{doc_names[0]}</th>\n",
    "                <th>{doc_names[1]}</th>\n",
    "                </tr>\n",
    "                \"\"\"\n",
    "    \n",
    "    a,b,c,d,e = get_matching_regs(doc1, doc2)\n",
    "    \n",
    "    for ele in a:\n",
    "        i1, i2, i3 = ele\n",
    "\n",
    "        rid1 , s1 = list_d1[i2]\n",
    "        rid2 , s2 = lis\n",
    "        a1 = [\"\"]*len(s1)\n",
    "        a2 = [\"\"]*len(s2)\n",
    "        \n",
    "        f, p, n = match_two_regs_for_html(s1,s2)\n",
    "                \n",
    "        #full matches\n",
    "        for e1,e2 in f:\n",
    "            a1[e1] = f\"<p style='color:olive;'>{s1[e1]}</p>\"\n",
    "            a2[e2] = f\"<p style='color:olive;'>{s2[e2]}</p>\"\n",
    "        \n",
    "        # non matches\n",
    "        for e1 in n[0]:\n",
    "            a1[e1] = f\"<p style='color:red;'>{s1[e1]}</p>\"\n",
    "        for e2 in n[1]:\n",
    "            a2[e2] = f\"<p style='color:red;'>{s2[e2]}</p>\"\n",
    "        \n",
    "        #partial matches\n",
    "        for ele in p:\n",
    "            words1, words2, e1, e2, color_map1, color_map2 = ele\n",
    "            \n",
    "            lis1 = []\n",
    "            lis2 = []\n",
    "            \n",
    "            for word,color in zip(words1, color_map1):\n",
    "                color_string = \"olive\" if color == 0 else \"blue\"\n",
    "                lis1.append(f\"<span style='color:{color_string}'>{word}</span>\")\n",
    "            for word, color in zip(words2, color_map2):\n",
    "                color_string = \"olive\" if color == 0 else \"blue\"\n",
    "                lis2.append(f\"<span style='color:{color_string}'>{word}</span>\")\n",
    "            a1[e1] = f\"<p>{' '.join(lis1)}</p>\"\n",
    "            a2[e2] = f\"<p>{' '.join(lis2)}</p>\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        \n",
    "        html_obj += f\"\"\"<tr>\n",
    "        <td>{\"\".join(a1)}</td>\n",
    "        <td>{\"\".join(a2)}</td>\n",
    "        </tr>\"\"\"    \n",
    "    \n",
    "    for ele in d:\n",
    "        html_obj += f\"\"\"<tr>\n",
    "        <td>{list_d1[ele]}</td>\n",
    "        <td></td>\n",
    "        </tr>\"\"\"\n",
    "\n",
    "    for ele in e:\n",
    "        html_obj += f\"\"\"<tr>\n",
    "        <td></td>\n",
    "        <td>{list_d2[ele]}</td>\n",
    "        </tr>\"\"\"\n",
    "    \n",
    "    html_obj += \"</table>\"\n",
    "    \n",
    "    return html_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returned_html = generate_html(list_d1, list_d2)\n",
    "# print(sorted(key_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key1, key2 = 'PIT_2003','PIT_2015'\n",
    "# d1 = datum[key1]\n",
    "# d2 = datum[key2]\n",
    "# list_d1 = nested_list_to_linear_list(list(d1.values()))\n",
    "# list_d2 = nested_list_to_linear_list(list(d2.values()))\n",
    "\n",
    "# list_d1 = list_d1[2:]\n",
    "# list_d2 = list_d2[2:]\n",
    "# returned_html = generate_html(list_d1, list_d2,doc_names=[key1,key2])\n",
    "# display(HTML(returned_html))\n",
    "# display(HTML(\"<br/>\"))\n",
    "\n",
    "# for ele in list_d1:\n",
    "#     print(ele, \"~~~\")\n",
    "# print('---------')\n",
    "# for ele in list_d2:\n",
    "#     print(ele,\"~~~\")\n",
    "\n",
    "# key_list = sorted([ele for ele in list(datum.keys()) if 'PIT' in ele])\n",
    "\n",
    "# # for i,key1 in enumerate(key_list):\n",
    "# #     for key2 in key_list[i+1:]:\n",
    "# #         print(key1,key2)\n",
    "\n",
    "# returned_html = \"\"\n",
    "\n",
    "# for i,key1 in enumerate(key_list[:-1]):\n",
    "#         key2 = key_list[i+1]\n",
    "#         print(key1,key2)\n",
    "#         d1 = datum[key1]\n",
    "#         d2 = datum[key2]\n",
    "#         list_d1 = nested_list_to_linear_list(list(d1.values()))\n",
    "#         list_d2 = nested_list_to_linear_list(list(d2.values()))\n",
    "\n",
    "#         list_d1 = list_d1[2:]\n",
    "#         list_d2 = list_d2[2:]\n",
    "        \n",
    "#         returned_html += generate_html(list_d1, list_d2,doc_names=[key1,key2]) + \"<br/>\"\n",
    "# display(HTML(returned_html))\n",
    "# with open('../../data/FOR_SENDING/all_combos.html','w') as f:\n",
    "#     f.write(returned_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stricter\n",
    "\n",
    "Lenient\n",
    "\n",
    "-- ? --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import re\n",
    "\n",
    "def process_regs_list(d1): \n",
    "    final_string = []\n",
    "    for reg_num, sub_reg_num, cont in d1:\n",
    "        final_string.append([reg_num,cont])\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_date_string(date,doc_type): \n",
    "    dm,y = date.split('_')\n",
    "    m,d = dm[:3], dm[3:]\n",
    "    return f\"{doc_type} {d}-{m}-{y}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_html_for_amendments(list_d1, list_d2, doc_names=None, doc_type=None):\n",
    "    if doc_names is None:\n",
    "        doc_names = [\"Document 1\", \"Document 2\"]\n",
    "    \n",
    "    doc_names_0 = split_date_string(doc_names[0] , doc_type)\n",
    "    doc_names_1 = split_date_string(doc_names[1] , doc_type)\n",
    "    html_obj = f\"\"\"\n",
    "                <html>\n",
    "                <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css\">\n",
    "\n",
    "                <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
    "\n",
    "                <script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js\"></script>\n",
    "                \n",
    "                <style>\n",
    "                ul.myList {{\n",
    "                  list-style-type:disc;\n",
    "                }}\n",
    "                \n",
    "                </style>\n",
    "                <body>\n",
    "                <br/>\n",
    "\n",
    "                <div style=\"position: sticky; top: 0;\">\n",
    "                \n",
    "                <ul>\n",
    "                \n",
    "                <li style='color:olive;'> <a href=\"#exactm\"> Exact Match </a></li>\n",
    "                \n",
    "                <li style=\"list-style-type: none;\"> Insertions or Deletions\n",
    "\n",
    "                <ul class=\"myList\">\n",
    "                \n",
    "                <li style='color:red;'> <a href=\"#partm\">Subregulation</a> </li>\n",
    "                <li style='color:black;'> <a href=\"#nomen\"> Regulation</a></li >\n",
    "                \n",
    "                </ul>\n",
    "                </li>\n",
    "                \n",
    "                <li style='color:blue;'>  <a href=\"#partm\"> Edits </a></li>\n",
    "                \n",
    "                </ul>\n",
    "                </div>\n",
    "                \n",
    "                \n",
    "                <div class=\"container pt-3\">\n",
    "                <table class=\"table table-hover table-bordered\">\n",
    "                <tr>\n",
    "                <th>{doc_names_0}</th>\n",
    "                <th>{doc_names_1}</th>\n",
    "                </tr>\n",
    "                <div id=\"exactm\"></div>\n",
    "                \"\"\"\n",
    "    \n",
    "    a,b,c,d,e = get_matching_regs(list_d1, list_d2)\n",
    "    \n",
    "#     html_obj += ''\n",
    "    red_blue_flag = True\n",
    "    \n",
    "    for ele in a:\n",
    "        i1, i2, i3 = ele\n",
    "        \n",
    "        rid1 , s1 = list_d1[i2]\n",
    "        rid2 , s2 = list_d2[i3]\n",
    "        \n",
    "#         s1 = [ele.replace('\\n',\"\") for i,ele in enumerate((list_d1[i2])) if i == 0 \n",
    "#                   or (len(ele) > 10 and 'Inserted' not in ele and 'Substituted' not in ele and 'NOTE' not in ele and 'Omitted' not in ele)]\n",
    "#         s2 = [ele.replace('\\n','') for i,ele in enumerate((list_d2[i3])) if i == 0\n",
    "#                   or (len(ele) > 10 and 'Inserted' not in ele and 'Substituted' not in ele and 'NOTE' not in ele and 'Omitted' not in ele)]\n",
    "                \n",
    "        a1 = [\"\"]*len(s1)\n",
    "        a2 = [\"\"]*len(s2)\n",
    "        \n",
    "        f, p, n = match_two_regs_for_html(s1,s2)\n",
    "                \n",
    "        #full matches\n",
    "        for e1,e2 in f:\n",
    "            a1[e1] = f\"<p style='color:olive;'>{s1[e1]}</p>\"\n",
    "            a2[e2] = f\"<p style='color:olive;'>{s2[e2]}</p>\"\n",
    "        \n",
    "        # non matches\n",
    "        \n",
    "        for e1 in n[0]:\n",
    "            if red_blue_flag == True:\n",
    "                a1[e1] = f\" <div id='partm'> <p style='color:red;'>{s1[e1]}</p> </div> \"\n",
    "                red_blue_flag = False\n",
    "            else:\n",
    "                a1[e1] = f\"<p style='color:red;'>{s1[e1]}</p>\"\n",
    "\n",
    "        for e2 in n[1]:\n",
    "            if red_blue_flag == True:\n",
    "                a2[e2] = f\"<div id='partm'><p style='color:red;'>{s2[e2]}</p></div>\"\n",
    "                red_blue_flag = False\n",
    "            else : a2[e2] = f\"<p style='color:red;'>{s2[e2]}</p>\"\n",
    "        \n",
    "        #partial matches\n",
    "        for ele in p:\n",
    "            words1, words2, e1, e2, color_map1, color_map2 = ele\n",
    "            \n",
    "            lis1 = []\n",
    "            lis2 = []\n",
    "            \n",
    "            for word,color in zip(words1, color_map1):\n",
    "                color_string = \"olive\" if color == 0 else \"blue\"\n",
    "                lis1.append(f\"<span style='color:{color_string}'>{word}</span>\")\n",
    "            for word, color in zip(words2, color_map2):\n",
    "                color_string = \"olive\" if color == 0 else \"blue\"\n",
    "                lis2.append(f\"<span style='color:{color_string}'>{word}</span>\")\n",
    "            a1[e1] = f\"<p>{' '.join(lis1)}</p>\"\n",
    "            a2[e2] = f\"<p>{' '.join(lis2)}</p>\"\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        \n",
    "        html_obj += f\"\"\"<tr>\n",
    "        <td>{rid1} {\"\".join(a1)}</td>\n",
    "        <td>{rid2} {\"\".join(a2)}</td>\n",
    "        </tr>\"\"\"    \n",
    "        \n",
    "    check_nonem = True\n",
    "    \n",
    "    for i,ele in enumerate(d):\n",
    "        rid, con = list_d1[ele]\n",
    "        if check_nonem == True:\n",
    "            html_obj += f\"\"\"<tr>\n",
    "                    <td><div id=\"nomen\">{rid} {\"<br/>\".join(con)}</div></td>\n",
    "                    <td></td>\n",
    "                    </tr>\"\"\"\n",
    "            check_nonem = False\n",
    "        else:\n",
    "            html_obj += f\"\"\"<tr>\n",
    "            <td>{rid} {\"<br/>\".join(con)}</td>\n",
    "            <td></td>\n",
    "            </tr>\"\"\"\n",
    "\n",
    "    for ele in e:\n",
    "        rid, con = list_d2[ele]\n",
    "        if check_nonem == True:\n",
    "            html_obj += f\"\"\"<tr>\n",
    "                    <td></td>\n",
    "                    <td><div id=\"nomen\">{rid} {\"<br/>\".join(con)}</div></td>\n",
    "                    </tr>\"\"\"\n",
    "            check_nonem = False\n",
    "        else:\n",
    "            html_obj += f\"\"\"<tr>\n",
    "            <td></td>\n",
    "             <td>{rid} {\"<br/>\".join(con)}</td>\n",
    "            </tr>\"\"\"\n",
    "    \n",
    "    html_obj += \"</div></body></table></html>\"\n",
    "    \n",
    "    return html_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comparision_list(list_d1, list_d2, doc_names=None, doc_type=None):\n",
    "    if doc_names is None:\n",
    "        doc_names = [\"Document 1\", \"Document 2\"]\n",
    "    \n",
    "    doc_names_0 = split_date_string(doc_names[0] , doc_type)\n",
    "    doc_names_1 = split_date_string(doc_names[1] , doc_type)\n",
    "    \n",
    "    \n",
    "    a,b,c,d,e = get_matching_regs(list_d1, list_d2)\n",
    "    \n",
    "    red_blue_flag = True\n",
    "    compare_arr = []\n",
    "    \n",
    "    for ele in a:\n",
    "        \n",
    "        i1, i2, i3 = ele\n",
    "        \n",
    "        rid1 , s1 = list_d1[i2]\n",
    "        rid2 , s2 = list_d2[i3]\n",
    "        \n",
    "        compare_arr.append([s1,s2,rid1,rid2])\n",
    "    \n",
    "    for i,ele in enumerate(d):\n",
    "        rid, con = list_d1[ele]\n",
    "        compare_arr.append([con, \"\", rid, \"0\"])\n",
    "\n",
    "    for ele in e:\n",
    "        rid, con = list_d2[ele]\n",
    "        compare_arr.append([\"\",con,\"0\",rid])\n",
    "    \n",
    "    \n",
    "    return compare_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def standalone_html(list_d1, doc_names=None, doc_type=None):\n",
    "    if doc_names is None:\n",
    "        doc_names = [\"Document 1\"]\n",
    "    \n",
    "    doc_names_0 = split_date_string(doc_names[0] , doc_type)\n",
    "    count_tot = sum([(match != \"\") for _,_,match in list_d1])\n",
    "    count_string = \"\".join([f\"\"\"<li style='color:olive;'> <a href=\"#rationale{it}\"> Rationale {it} </a></li>\"\"\" for it in range(count_tot)])\n",
    "    \n",
    "    html_obj = f\"\"\"\n",
    "                <html>\n",
    "                <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css\">\n",
    "\n",
    "                <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
    "\n",
    "                <script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js\"></script>\n",
    "                \n",
    "                <style>\n",
    "                ul.myList {{\n",
    "                  list-style-type:disc;\n",
    "                }}\n",
    "                \n",
    "                </style>\n",
    "                \n",
    "                <body>\n",
    "                <br/>\n",
    "                \n",
    "                <div class=\"container pt-3\">\n",
    "                <table class=\"table table-hover table-bordered\">\n",
    "                \"\"\" + \"\"\"<div style=\"position: sticky; top: 0;\">\n",
    "                <ul>\"\"\" +  count_string + \"\"\"</ul>\n",
    "                </div>\"\"\" + f\"\"\"\n",
    "                <tr>\n",
    "                <th>{doc_names_0}</th>\n",
    "                </tr>\n",
    "                \"\"\"\n",
    "    count = 0\n",
    "    for rid, ele, match in list_d1:\n",
    "        condition_string = \"\"\n",
    "        if match != \"\":\n",
    "            condition_string = f\"\"\"<br/><br/><p style='color:blue;' id=\"rationale{count}\">Rationale : {match}</p>\"\"\"\n",
    "            count += 1\n",
    "        html_obj += f\"\"\"<tr><td>{rid} {\" \".join(ele)}\"\"\" + condition_string + \"\"\"</td></tr>\"\"\"\n",
    "    \n",
    "    html_obj += \"</div></body></table></html>\"\n",
    "    \n",
    "    return html_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_standalone_list(list_d1, doc_names=None, doc_type=None):\n",
    "    if doc_names is None:\n",
    "        doc_names = [\"Document 1\"]\n",
    "    \n",
    "    \n",
    "    answer = []\n",
    "    for rid, ele in list_d1:\n",
    "        answer.append([rid,\" \".join(ele)])\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../data/FOR_SENDING/amednments_comparision.html','w') as f:\n",
    "#     f.write(returned_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# \"\"\"\n",
    "# for file in glob('../home/buggi/data')\n",
    "# with open('/home/buggi/RA/data/FOR_SENDING/all_pit_regulations.json','r') as f:\n",
    "#     all_pit = json.load(f)\n",
    "\n",
    "# from collections import defaultdict\n",
    "# all_pit_cleaned = {}\n",
    "\n",
    "# for ele in all_pit:\n",
    "#     print(ele)\n",
    "#     all_pit_cleaned[ele] = []\n",
    "#     keyl = list(all_pit[ele].keys())[1:]\n",
    "#     for chap in keyl:\n",
    "#         for reg in all_pit[ele][chap]:\n",
    "#             if reg != 'chap_title':\n",
    "#                 content = all_pit[ele][chap][reg][\"content\"]\n",
    "#                 if type(content) is str:\n",
    "#                     cont = content\n",
    "#                 else:\n",
    "#                     cont = \" \".join(content)\n",
    "#                 all_pit_cleaned[ele].append([reg,\"\",cont.split(\".\",1)[1].strip()])\n",
    "\n",
    "# with open('/home/buggi/RA/data/FOR_SENDING/all_pit_supressed.json','w') as f:\n",
    "#     json.dump(all_pit_cleaned,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pit_cleaned.keys()\n",
    "from datetime import datetime\n",
    "def date_convo(date_string):\n",
    "    elem = date_string\n",
    "    if '.' in date_string:\n",
    "        elem = date_string.split('.')[0]\n",
    "    dt1 = datetime.strptime(elem, '%b%d_%Y')\n",
    "    return dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running code for Mutual Funds comparision\n",
    "# import json\n",
    "# with open('/home/buggi/Downloads/Prohibition_insider_trading.json') as file:\n",
    "#     clatum = json.load(file)\n",
    "\n",
    "# all_pit_cleaned = {ele : all_pit_cleaned[ele] for ele in all_pit_cleaned if \"2019\" not in all_pit_cleaned}\n",
    "\n",
    "# datum = {}\n",
    "\n",
    "# for ele in all_pit_cleaned:\n",
    "#     datum[ele] = all_pit_cleaned[ele]\n",
    "# for ele in clatum:\n",
    "#     datum[ele] = clatum[ele]\n",
    "\n",
    "# key_list = ['Aug7_2003', 'Aug16_2011', 'Jan15_2015', 'Dec31_2018', 'Jan21_2019', 'Sept17_2019', 'Nov11_2019', 'July17_2020', 'Oct29_2020']\n",
    "\n",
    "# dc = {}\n",
    "# for ele in key_list:\n",
    "#     dc[ele] = datum[ele]\n",
    "\n",
    "# with open('/home/buggi/RA/data/FOR_SENDING/all_pit_combined.json','r') as f:\n",
    "#     datum = json.load(f)\n",
    "if True == False:\n",
    "    datum = {}\n",
    "    from itertools import groupby\n",
    "\n",
    "    key_list = list(datum.keys())\n",
    "    print(key_list)\n",
    "\n",
    "    def better_key(keyn):\n",
    "        if \"Sept\" in keyn:\n",
    "            return keyn.replace(\"Sept\",\"Sep\")\n",
    "        elif \"July\" in keyn:\n",
    "            return keyn.replace(\"July\",\"Jul\")\n",
    "        else:\n",
    "            return keyn\n",
    "\n",
    "    for i,key1 in enumerate(key_list[:-1]):\n",
    "            key2 = key_list[i+1]\n",
    "            d1 = datum[key1]\n",
    "            d2 = datum[key2]\n",
    "\n",
    "            list_d1 = process_regs_list(d1)\n",
    "            list_d2 = process_regs_list(d2)\n",
    "            generated_html_comparer = generate_html_for_amendments(list_d1, list_d2,doc_names=[better_key(key1),better_key(key2)],doc_type=\"PIT Regulations\")\n",
    "            with open(f\"../demo/All_HTMLS/PIT_Regs/comparision/{better_key(key1)}vs{better_key(key2)}.html\",'w') as f:\n",
    "                f.write(generated_html_comparer)\n",
    "\n",
    "    for i,key1 in enumerate(key_list):\n",
    "            d1 = datum[key1]\n",
    "\n",
    "            list_d1 = process_regs_list(d1)        \n",
    "            standalone_html_comparer = standalone_html(list_d1, doc_names=[better_key(key1)], doc_type=\"PIT Regulations\")\n",
    "            with open(f\"../demo/All_HTMLS/PIT_Regs/standalone/{better_key(key1)}.html\",'w') as f:\n",
    "                f.write(standalone_html_comparer)\n",
    "\n",
    "    # Running code for Mutual Funds comparision\n",
    "    import json\n",
    "    with open('/home/buggi/Downloads/Mutual_Funds.json') as file:\n",
    "        datum = json.load(file)\n",
    "\n",
    "    key_list = list(datum.keys())\n",
    "\n",
    "    returned_html = \"\"\n",
    "    from itertools import groupby\n",
    "\n",
    "    for i,key1 in enumerate(key_list[:-1]):\n",
    "            key2 = key_list[i+1]\n",
    "            d1 = datum[key1]\n",
    "            d2 = datum[key2]\n",
    "\n",
    "            list_d1 = process_regs_list(d1)\n",
    "            list_d2 = process_regs_list(d2)\n",
    "\n",
    "            generated_html_comparer = generate_html_for_amendments(list_d1, list_d2,doc_names=[better_key(key1),better_key(key2)],doc_type=\"Mutual Funds Regulations\")\n",
    "            with open(f\"../demo/All_HTMLS/Mutual_Funds_Regs/comparision/{better_key(key1)}vs{better_key(key2)}.html\",'w') as f:\n",
    "                f.write(generated_html_comparer)\n",
    "\n",
    "    for i,key1 in enumerate(key_list):\n",
    "            d1 = datum[key1]\n",
    "\n",
    "            list_d1 = process_regs_list(d1)        \n",
    "            standalone_html_comparer = standalone_html(list_d1, doc_names=[better_key(key1)],doc_type=\"Mutual Funds Regulations\")\n",
    "            with open(f\"../demo/All_HTMLS/Mutual_Funds_Regs/standalone/{better_key(key1)}.html\",'w') as f:\n",
    "                f.write(standalone_html_comparer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def get_file_name(url, ext):\n",
    "    return url.split('/')[-1].split('.')[0] + ext\n",
    "\n",
    "\n",
    "def get_file_name_hashed(url, ext):\n",
    "    return hashlib.md5(url.encode()).hexdigest() + ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../document_scraping/script_folder/document_all_circulars.json\n",
      "../document_scraping/script_folder/document_all_debt_offer_documents_draft_filed_with_se.json\n",
      "../document_scraping/script_folder/document_all_acts.json\n",
      "../document_scraping/script_folder/document_all_rules.json\n",
      "../document_scraping/script_folder/document_all_public_issues_other_documents.json\n",
      "../document_scraping/script_folder/document_all_final_placement_memorandum_filed_with_sebi.json\n",
      "../document_scraping/script_folder/document_all_invit_public_offer_docs_filed_with_sebi.json\n",
      "../document_scraping/script_folder/document_all_investor_survey.json\n",
      "../document_scraping/script_folder/document_all_placement_memorandum_filed_with_sebi.json\n",
      "../document_scraping/script_folder/document_all_filings_processing_status.json\n",
      "../document_scraping/script_folder/document_all_takeovers_other_documents.json\n",
      "../document_scraping/script_folder/document_all_annual_reports.json\n",
      "../document_scraping/script_folder/document_all_offer_documents_filled_with_sebi.json\n",
      "../document_scraping/script_folder/document_all_reit_draft_offer_document_filed_with_sebi.json\n",
      "../document_scraping/script_folder/document_all_rights_issues_draft_letters_sebi.json\n",
      "../document_scraping/script_folder/document_all_public_issues_red_herring_documents_filed_with_sebi.json\n",
      "../document_scraping/script_folder/document_all_draft_offer_documents_filled_with_sebi.json\n",
      "../document_scraping/script_folder/document_all_final_offer_documents_filled_with_sebi.json\n",
      "../document_scraping/script_folder/document_all_mutual_funds_draft_filings.json\n",
      "../document_scraping/script_folder/document_all_unserved_summons_or_notices.json\n",
      "../document_scraping/script_folder/document_all_press_releases.json\n",
      "../document_scraping/script_folder/document_all_public_notices.json\n",
      "../document_scraping/script_folder/document_all_filings_scheme_of_arrangement.json\n",
      "../document_scraping/script_folder/document_all_gazette_notification.json\n",
      "../document_scraping/script_folder/document_all_speeches.json\n",
      "../document_scraping/script_folder/document_all_annual_accounts.json\n",
      "../document_scraping/script_folder/document_all_buybacks_tender_offers.json\n",
      "../document_scraping/script_folder/document_all_latest_regulations.json\n",
      "../document_scraping/script_folder/document_all_regs.json\n",
      "../document_scraping/script_folder/document_all_recovery_proceedings.json\n",
      "../document_scraping/script_folder/document_all_notice_for_meeting_on_schemes.json\n",
      "../document_scraping/script_folder/document_all_public_issues_final_offer_documents_filed_with_roc.json\n",
      "../document_scraping/script_folder/document_all_invit_public_draft_offer_docs_filed_with_sebi.json\n",
      "../document_scraping/script_folder/document_all_monthly_sebi_bulletin.json\n",
      "../document_scraping/script_folder/document_all_reports_and_notifications.json\n",
      "../document_scraping/script_folder/document_all_rights_issues_final_letters_stock_exchanges.json\n",
      "../document_scraping/script_folder/document_all_reports_for_public_comments.json\n",
      "../document_scraping/script_folder/document_all_auction_notice_under_recovery_proceedings.json\n",
      "../document_scraping/script_folder/document_all_glossary.json\n",
      "../document_scraping/script_folder/document_all_handbook_of_statistics.json\n",
      "../document_scraping/script_folder/document_all_buybacks_open_market_through_stock_exchanges.json\n",
      "../document_scraping/script_folder/document_all_public_issues_draft_offer_documents_filed_with_sebi.json\n",
      "../document_scraping/script_folder/document_all_filings_issues.json\n",
      "../document_scraping/script_folder/document_all_working_papers.json\n",
      "../document_scraping/script_folder/document_all_news_clarifications.json\n",
      "../document_scraping/script_folder/document_all_invit_final_offer_docs_filed_with_sebi.json\n",
      "../document_scraping/script_folder/document_all_sebi_drg_studies.json\n",
      "../document_scraping/script_folder/document_all_orders_that_couldnt_be_served.json\n",
      "../document_scraping/script_folder/document_all_debt_offer_documents_final_filed_with_roc.json\n",
      "../document_scraping/script_folder/document_all_buybacks.json\n",
      "../document_scraping/script_folder/document_all_unsc_sanctions_committee_list.json\n",
      "../document_scraping/script_folder/document_all_informal_guidelines.json\n",
      "../document_scraping/script_folder/document_all_master_circulars.json\n",
      "../document_scraping/script_folder/document_all_reit_final_offer_document_filed_with_sebi.json\n",
      "../document_scraping/script_folder/document_all_reit_offer_document_filed_with_sebi.json\n",
      "../document_scraping/script_folder/document_all_international_research_conference.json\n",
      "../document_scraping/script_folder/document_all_rights_issues_other_documents.json\n",
      "../document_scraping/script_folder/document_all_committee_reports.json\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "all_files_dict = {}\n",
    "for file in glob('../document_scraping/script_folder/document_all_*.json'):\n",
    "    print(file)\n",
    "    with open(file,'r') as f:\n",
    "        jsonf = json.load(f)\n",
    "    for ele in jsonf:\n",
    "        try:\n",
    "            if ele[\"file_url\"] != None:\n",
    "                all_files_dict[get_file_name(ele[\"file_url\"],\"\")] = {\"time\": ele[\"time\"],\n",
    "                                                                    \"text\": ele[\"text\"],\n",
    "                                                                    \"url\" : ele[\"url\"],\n",
    "                                                                     \"base\": file}\n",
    "\n",
    "            else:\n",
    "                all_files_dict[get_file_name_hashed(ele[\"url\"],\"\")] = {\"time\": ele[\"time\"],\n",
    "                                                        \"text\": ele[\"text\"],\n",
    "                                                        \"url\" : ele[\"url\"],\n",
    "                                                        \"base\": file}\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def references_html(list_d1, doc_names=None, doc_type=None):\n",
    "    if doc_names is None:\n",
    "        doc_names = [\"Document 1\"]\n",
    "    \n",
    "    doc_names_0 = split_date_string(doc_names[0] , doc_type)\n",
    "    html_obj = f\"\"\"\n",
    "                <html>\n",
    "                <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css\">\n",
    "\n",
    "                <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
    "\n",
    "                <script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js\"></script>\n",
    "                <body>\n",
    "                <br/>\n",
    "                \n",
    "                <div class=\"container pt-3\">\n",
    "                <table class=\"table table-hover table-bordered\">\n",
    "                <tr>\n",
    "                <th colspan=\"3\" class=\"text-center\">{doc_names_0}</th>\n",
    "                </tr>\n",
    "                \n",
    "                <tr>\n",
    "                <th>Time</th>\n",
    "                <th>Document</th>\n",
    "                <th>Match </th>\n",
    "                </tr>\n",
    "                \"\"\"\n",
    "    # print(list_d1[:3])\n",
    "    \n",
    "    list_d1.sort(key=lambda x: x[\"dtm\"])\n",
    "    for ele in list_d1:\n",
    "        html_obj += f\"\"\"<tr>\n",
    "        <td>{ele[\"time\"]}</td>\n",
    "        <td><a href='{ele[\"url\"]}'>{ele[\"text\"]}</a></td>\n",
    "        <td>{ele[\"content\"]}</td>\n",
    "        </tr>\"\"\"\n",
    "    \n",
    "    html_obj += \"</div></body></table></html>\"\n",
    "    \n",
    "    return html_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dups = lambda x : list(set(x)) if type(x) is list else x\n",
    "def clean_grep_out(inp):\n",
    "    all_rows = inp.strip().split('\\n')\n",
    "    \n",
    "    first_row = all_rows[0]\n",
    "    file_name = first_row.split('-')[0] if '-' in first_row else first_row.split(':')[0]\n",
    "    len_file_name_p = len(file_name) \n",
    "    \n",
    "    cleaned_all_rows = \" \".join([ele[len_file_name_p+1:] for ele in all_rows if \"Page\" not in ele])                                \n",
    "    \n",
    "    fn = os.path.basename(file_name).split(\".\")[0]\n",
    "    return fn, cleaned_all_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def detect_pattern(content,other_patterns):\n",
    "    pattern = re.compile(\"amendment.*rationale|amendment.*rationale|regulation.*amendment|amendment.*regulation|SEBI \\(.*?\\) Regulations\" + \"|\"+\n",
    "                        \"|\".join(other_patterns))\n",
    "    all_matches = []\n",
    "    final_content = \"\"\n",
    "    i = 0\n",
    "    for match in re.finditer(pattern,content):\n",
    "        if i == 0:\n",
    "            final_content += content[i:match.start()]\n",
    "        final_content += f\"<mark>{content[match.start():match.end()]}</mark>\"\n",
    "        i = match.end()\n",
    "    final_content += content[i:]\n",
    "    return final_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../data/glossary.json','r') as f:\n",
    "#     glossary = json.load(f)\n",
    "# with open('../../data/all_reguls_list.json','r') as f:\n",
    "#     regs_names = json.load(f)\n",
    "\n",
    "# file_names = [os.path.basename(file).split('.')[0].replace('_',' ') for file in tqdm(sorted(glob('../../data/ALL_REGULATIONS_JSON_FLATTENED/*.json')))]\n",
    "# # print(file_names)\n",
    "\n",
    "# dict_fretaly = {}\n",
    "# glossary_reved = {}\n",
    "\n",
    "# for key in glossary:\n",
    "#     for elem in glossary[key]:\n",
    "#         if elem in glossary_reved:\n",
    "#             glossary_reved[elem].append(key)\n",
    "#         else:\n",
    "#             glossary_reved[elem] = [key]\n",
    "\n",
    "# for ele in glossary_reved:\n",
    "#     key = regs_names[ele]\n",
    "#     dict_fretaly[key] = glossary_reved[ele] + [key.lower()]\n",
    "\n",
    "# print(dict_fretaly)\n",
    "# with open('../../data/filename_to_refs.json','w') as f:\n",
    "#     json.dump(dict_fretaly)\n",
    "\n",
    "# from custom_functions import write_file\n",
    "# filename_to_refs = write_file('../../data/filename_to_refs.json',dict_fretaly)\n",
    "\n",
    "with open('../../data/filename_to_refs.json','r') as f:\n",
    "    filename_to_refs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [00:00, 846.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Underwriters.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Substantial_Acquisition_of_Shares_and_Takeovers.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Stock_Exchanges_and_Clearing_Corporations.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Stock_Brokers.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Share_Based_Employee_Benefits.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Settlement_of_Administrative_and_Civil_Proceedings.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Settlement_Proceedings.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Self_Regulatory_Organisations.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Research_Analysts.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Regulatory_Fee_on_Stock_Exchanges.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Registrars_to_an_Issue_and_Share_Transfer_Agents.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Real_Estate_Investment_Trusts.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Prohibition_of_Insider_Trading.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Prohibition_of_Fraudulent_and_Unfair_Trade_Practices_relating_to_Securities_Market.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Procedure_of_Search_and_Seizure.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Procedure_For_Board_Meetings.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Portfolio_Managers.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Ombudsman.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Mutual_Funds.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Merchant_Bankers.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Listing_Obligations_and_Disclosure_Requirements.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Know_Your_Client_Regulations.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Issues_and_Listing_of_Muncipal_Debt_Securities.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Issue_of_Sweat_Equity.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Issue_of_Capital_and_Disclosure_Requirements.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Issue_and_Listing_of_Non_Convertible_Redeemable_Preference_Shares.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Issue_and_Listing_of_Debt_Securities.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Issue_and_Listing__of_Securities_Debt_Instruments_and_Security_Receipts.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Investor_Protection_and_Education_Fund.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Investment_Advisers.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Intermediaries.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Infrastructure_Investment_Trusts.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Foreign_Venture_Capital_Investor.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Foreign_Portfolio_Investors.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Employees_Service.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Depositories_and_Participants.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Delisting_of_Equity_Shares.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Debenture_Trustees.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Custodian.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Credit_Rating_Agencies.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Collective_Investment_Scheme.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Certification_of_Associated_Persons_in_the_Securities_Markets.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Central_Database_of_Market_Participants.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Buy_Back_Of_Securities.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Bankers_to_an_Issue.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Appointment_of_Administrator_and_Procedure_for_Refunding_to_the_Investors.json\n",
      "../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/Alternative_Investment_Funds.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Iterates all jsons and generates HTML\n",
    "from glob import glob\n",
    "from itertools import groupby\n",
    "output_folder = '../../data/ALL_REGULATIONS_HTML/'\n",
    "from tqdm import tqdm as tqdm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "STANDALONES = True\n",
    "COMPARISION = False\n",
    "COMMENTS = False\n",
    "COMPARISION_CONTENT = False\n",
    "STANDALONE_CONTENT  = False\n",
    "\n",
    "with open('../../data/regulation_rationale_match.pkl','rb') as f:\n",
    "    reg_rat_dict = pickle.load(f)\n",
    "# print(reg_rat_dict)\n",
    "\n",
    "for file in tqdm(reversed(sorted(glob('../../data/ALL_REGULATIONS_JSON_FLATTENED_SPLIT_SUBREG/*.json')))):\n",
    "    print(file)\n",
    "    with open(file,'r') as f:\n",
    "        datum = json.load(f)\n",
    "    \n",
    "    key_list = sorted(list(datum.keys()), key=date_convo)\n",
    "    \n",
    "    folder_name = os.path.basename(file).split('.')[0]\n",
    "    \n",
    "    output_subfolder = os.path.join(output_folder, folder_name)\n",
    "    \n",
    "    if not os.path.exists(output_subfolder):\n",
    "        os.mkdir(output_subfolder)\n",
    "    \n",
    "    doc_t = \" \".join(folder_name.split('_'))\n",
    "    \n",
    "    if COMPARISION_CONTENT == True:\n",
    "        for i,key1 in enumerate(key_list[:-1]):\n",
    "                key2 = key_list[i+1]\n",
    "                d1 = datum[key1]\n",
    "                d2 = datum[key2]\n",
    "                \n",
    "                list_d1 = process_regs_list(d1)\n",
    "                list_d2 = process_regs_list(d2)\n",
    "                \n",
    "                generated_html_comparer_arr = generate_comparision_list(list_d1, list_d2,doc_names=[key1,key2],doc_type=doc_t)\n",
    "                fol_name = os.path.join(output_subfolder,'comparision_content')\n",
    "                if not os.path.exists(fol_name):\n",
    "                    os.mkdir(fol_name)\n",
    "\n",
    "                with open(os.path.join(fol_name,f\"{key1}vs{key2}.json\"),'w') as f:\n",
    "                    json.dump(generated_html_comparer_arr,f)\n",
    "                    \n",
    "    if COMPARISION == True:\n",
    "        for i,key1 in enumerate(key_list[:-1]):\n",
    "                key2 = key_list[i+1]\n",
    "                d1 = datum[key1]\n",
    "                d2 = datum[key2]\n",
    "                \n",
    "                list_d1 = process_regs_list(d1)\n",
    "                list_d2 = process_regs_list(d2)\n",
    "                \n",
    "                generated_html_comparer = generate_html_for_amendments(list_d1, list_d2,doc_names=[key1,key2],doc_type=doc_t)\n",
    "                fol_name = os.path.join(output_subfolder,'comparision')\n",
    "                if not os.path.exists(fol_name):\n",
    "                    os.mkdir(fol_name)\n",
    "\n",
    "                with open(os.path.join(fol_name,f\"{key1}vs{key2}.html\"),'w') as f:\n",
    "                    f.write(generated_html_comparer)\n",
    "    \n",
    "    if STANDALONES == True:\n",
    "        \n",
    "        for i,key1 in enumerate(key_list):\n",
    "                d1 = datum[key1]\n",
    "\n",
    "                list_d1 = process_regs_list(d1) \n",
    "                new_list_d1 = []\n",
    "                for ele in list_d1:\n",
    "                    new_list_d1.append(ele + [\"\"])\n",
    "                \n",
    "                for rk in reg_rat_dict:\n",
    "                    if rk[0] == doc_t and rk[1] == key1:\n",
    "                        new_list_d1[rk[2]][2] = reg_rat_dict[rk]\n",
    "                \n",
    "                \n",
    "                standalone_html_comparer = standalone_html(new_list_d1, doc_names=[key1], doc_type=doc_t)\n",
    "                \n",
    "                fol_name = os.path.join(output_subfolder,'standalone')\n",
    "\n",
    "\n",
    "                if not os.path.exists(fol_name):\n",
    "                    os.mkdir(fol_name)\n",
    "\n",
    "                with open(os.path.join(fol_name,f\"{key1}.html\"),'w') as f:\n",
    "                    f.write(standalone_html_comparer)\n",
    "    \n",
    "    if STANDALONE_CONTENT == True:\n",
    "        for i,key1 in enumerate(key_list):\n",
    "                d1 = datum[key1]\n",
    "\n",
    "                list_d1 = process_regs_list(d1)        \n",
    "                standalone_html_content = generate_standalone_list(list_d1, doc_names=[key1], doc_type=doc_t)\n",
    "                \n",
    "                fol_name = os.path.join(output_subfolder,'standalone_content')\n",
    "\n",
    "\n",
    "                if not os.path.exists(fol_name):\n",
    "                    os.mkdir(fol_name)\n",
    "\n",
    "                with open(os.path.join(fol_name,f\"{key1}.json\"),'w') as f:\n",
    "                    json.dump(standalone_html_content,f)\n",
    "                    \n",
    "    if COMMENTS == True:\n",
    "        list_ = []\n",
    "        import os\n",
    "        for reg_file in glob('../document_scraping/*.txt'):\n",
    "            with open(reg_file,'r') as f:\n",
    "                all_matches = f.read().split('--')\n",
    "                all_matches = [clean_grep_out(ele) for ele in all_matches]\n",
    "                key_for_glossary = os.path.basename(folder_name.replace('_',' '))\n",
    "                matchdct = filename_to_refs.get(key_for_glossary, [key_for_glossary.lower()]) \n",
    "                for filen, line in all_matches:    \n",
    "                    if any([ele in line for ele in matchdct]):\n",
    "                        line = detect_pattern(line,matchdct)\n",
    "                        list_.append((filen, line, reg_file))\n",
    "        from datetime import datetime \n",
    "        \n",
    "        final_dict = {}\n",
    "        set_indices = set()\n",
    "        key_lis_copy = key_list[:]\n",
    "        \n",
    "        # COPY THE DATE LIST\n",
    "        \n",
    "        while len(key_lis_copy) != 0:\n",
    "            key_date = key_lis_copy.pop(0)\n",
    "            dt1 = datetime.strptime(key_date, '%b%d_%Y')\n",
    "            final_dict[key_date] = []\n",
    "            \n",
    "            for i,ele in enumerate(list_):\n",
    "                if i in set_indices:\n",
    "                    continue\n",
    "\n",
    "                if ele[0] not in all_files_dict:\n",
    "                    set_indices.add(i)\n",
    "                    continue\n",
    "\n",
    "                tim = all_files_dict[ele[0]][\"time\"] \n",
    "                dt2 = datetime.strptime(tim, '%b %d, %Y')\n",
    "                if dt2 <= dt1:\n",
    "                    cpd_dict = all_files_dict[ele[0]]\n",
    "                    cpd_dict[\"content\"] = ele[1]\n",
    "                    cpd_dict[\"dtm\"] = dt2\n",
    "                    final_dict[key_date].append(cpd_dict)\n",
    "                    set_indices.add(i)\n",
    "        \n",
    "        \"HANDLE ADDING LEFT-OVERS\"\n",
    "        \n",
    "        fol_name = os.path.join(output_subfolder,'comments')\n",
    "        if not os.path.exists(fol_name):\n",
    "            os.mkdir(fol_name)\n",
    "                \n",
    "        for i,key1 in enumerate(key_list):\n",
    "            standalone_html_comparer = references_html(final_dict[key1], doc_names=[key1], doc_type=doc_t)\n",
    "            print_name = os.path.join(fol_name,f\"Comments_{key1}.html\")\n",
    "            with open(print_name,'w') as f:\n",
    "                f.write(standalone_html_comparer)\n",
    "                \n",
    "#     break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_ = []\n",
    "# import os\n",
    "# for reg_file in glob('../document_scraping/*.txt'):\n",
    "#     with open(reg_file,'r') as f:\n",
    "#         all_matches = f.read().split('--')\n",
    "#         all_matches = [clean_grep_out(ele) for ele in all_matches]\n",
    "#         for file, line in all_matches:\n",
    "#             matchdct = [pit]\n",
    "#             if any([ele in line for ele in matchdct]):\n",
    "#                 line = detect_pattern(line,matchdct)\n",
    "#                 list_.append((file, line, reg_file))\n",
    "\n",
    "# from datetime import datetime \n",
    "\n",
    "# count  = 0\n",
    "# for ele in list_:\n",
    "#     if ele not in all_files_dict:\n",
    "#         count += 1\n",
    "        \n",
    "# mfsl = ['Aug3_2006', 'Oct31_2007', 'Apr16_2008', 'Apr8_2009', 'Aug30_2011', 'Feb21_2012', 'Sep26_2012', 'Apr16_2013', 'Jun19_2013', 'Aug19_2013', 'May6_2014', 'Dec30_2014', 'May15_2015', 'Mar13_2018', 'Dec6_2018', 'Dec13_2018', 'Sep23_2019', 'Mar6_2020', 'Oct29_2020']\n",
    "\n",
    "# final_dict = {}\n",
    "# # setlis = set(list_)\n",
    "# set_indices = set()\n",
    "\n",
    "# while len(mfsl) != 0:\n",
    "#     key_date = mfsl.pop(0)\n",
    "#     dt1 = datetime.strptime(key_date, '%b%d_%Y')\n",
    "#     print(dt1)    \n",
    "#     final_dict[key_date] = []\n",
    "    \n",
    "    \n",
    "#     for i,ele in enumerate(list_):\n",
    "#         if i in set_indices:\n",
    "#             continue\n",
    "        \n",
    "#         # File not found\n",
    "#         if ele[0] not in all_files_dict:\n",
    "#             set_indices.add(i)\n",
    "#             continue\n",
    "        \n",
    "#         tim = all_files_dict[ele[0]][\"time\"] \n",
    "#         dt2 = datetime.s(tim, '%b %d, %Y')\n",
    "#         if dt2 <= dt1:\n",
    "#             cpd_dict = all_files_dict[ele[0]]\n",
    "#             cpd_dict[\"content\"] = ele[1]\n",
    "#             final_dict[key_date].append(cpd_dict)\n",
    "#             set_indices.add(i)\n",
    "            \n",
    "# key_list = ['Aug3_2006', 'Oct31_2007', 'Apr16_2008', 'Apr8_2009', 'Aug30_2011', 'Feb21_2012', 'Sept26_2012', 'Apr16_2013', 'Jun19_2013', 'Aug19_2013', 'May6_2014', 'Dec30_2014', 'May15_2015', 'Mar13_2018', 'Dec6_2018', 'Dec13_2018', 'Sept23_2019', 'Mar6_2020', 'Oct29_2020']\n",
    "\n",
    "# dte = {'Sep23_2019': 'Comments2',\n",
    "#  'Aug19_2013': 'Comments9',\n",
    "#  'Jun19_2013': 'Comments10',\n",
    "#  'Dec30_2014': 'Comments7',\n",
    "#  'Apr8_2009': 'Comments15',\n",
    "#  'Sep26_2012': 'Comments12',\n",
    "#  'Oct29_2020': 'Comments0',\n",
    "#  'Apr16_2013': 'Comments11',\n",
    "#  'Aug30_2011': 'Comments14',\n",
    "#  'May15_2015': 'Comments6',\n",
    "#  'Mar6_2020': 'Comments1',\n",
    "#  'Dec6_2018': 'Comments4',\n",
    "#  'Dec13_2018': 'Comments3',\n",
    "#  'Aug3_2006': 'Comments18',\n",
    "#  'Oct31_2007': 'Comments17',\n",
    "#  'Feb21_2012': 'Comments13',\n",
    "#  'May6_2014': 'Comments8',\n",
    "#  'Mar13_2018': 'Comments5',\n",
    "#  'Apr16_2008': 'Comments16'}\n",
    "\n",
    "# if False:\n",
    "#     for i,key1 in enumerate(key_list):\n",
    "#             bk = better_key(key1)\n",
    "#             standalone_html_comparer = references_html(final_dict[bk], doc_names=[better_key(key1)], doc_type=\"Mutual Funds Regulations\")\n",
    "\n",
    "#             with open(f\"../demo/All_HTMLS/Mutual_Funds_Regs/comments/{dte[better_key(key1)]}.html\",'w') as f:\n",
    "#                 f.write(standalone_html_comparer)\n",
    "                \n",
    "# list_ = []\n",
    "# import os\n",
    "# for reg_file in glob('../document_scraping/*.txt'):\n",
    "#     with open(reg_file,'r') as f:\n",
    "#         all_matches = f.read().split('--')\n",
    "#         all_matches = [clean_grep_out(ele) for ele in all_matches]\n",
    "#         for file, line in all_matches:\n",
    "#             matchdct = [\"Prohibition of  Insider  Trading\", \"PIT\",\"Insider Trading\",\"insider trading\"]\n",
    "#             if any([ele in line for ele in matchdct]):\n",
    "#                 line = detect_pattern(line,matchdct)\n",
    "#                 list_.append((file, line, reg_file))\n",
    "\n",
    "# print(len(list_))\n",
    "# from datetime import datetime \n",
    "\n",
    "# count  = 0\n",
    "# for ele in list_:\n",
    "#     if ele not in all_files_dict:\n",
    "#         count += 1\n",
    "# print(count, len(list_))        \n",
    "\n",
    "# pitsl = ['Aug7_2003', 'Aug16_2011', 'Jan15_2015', 'Dec31_2018', 'Jan21_2019', 'Sep17_2019', 'Nov11_2019', 'Jul17_2020', 'Oct29_2020'] \n",
    "\n",
    "# final_dict = {}\n",
    "# # setlis = set(list_)\n",
    "# set_indices = set()\n",
    "\n",
    "# while len(pitsl) != 0:\n",
    "#     key_date = pitsl.pop(0)\n",
    "#     dt1 = datetime.strptime(key_date, '%b%d_%Y')\n",
    "#     print(key_date,dt1)    \n",
    "#     final_dict[key_date] = []\n",
    "    \n",
    "    \n",
    "#     for i,ele in enumerate(list_):\n",
    "#         if i in set_indices:\n",
    "#             continue\n",
    "        \n",
    "#         # File not found\n",
    "#         if ele[0] not in all_files_dict:\n",
    "#             set_indices.add(i)\n",
    "#             continue\n",
    "        \n",
    "#         tim = all_files_dict[ele[0]][\"time\"] \n",
    "#         dt2 = datetime.strptime(tim, '%b %d, %Y')\n",
    "#         if dt2 <= dt1:\n",
    "#             cpd_dict = all_files_dict[ele[0]]\n",
    "#             cpd_dict[\"content\"] = ele[1]\n",
    "#             final_dict[key_date].append(cpd_dict)\n",
    "#             set_indices.add(i)\n",
    "            \n",
    "# key_list = ['Aug7_2003', 'Aug16_2011', 'Jan15_2015', 'Dec31_2018', 'Jan21_2019', 'Sept17_2019', 'Nov11_2019', 'July17_2020', 'Oct29_2020']\n",
    "\n",
    "# dte = {'Aug7_2003': 'Comments8',\n",
    "#  'Aug16_2011': 'Comments6',\n",
    "#  'Jan15_2015': 'Comments3',\n",
    "#  'Dec31_2018': 'Comments5',\n",
    "#  'Jan21_2019': 'Comments4',\n",
    "#  'Sep17_2019': 'Comments2',\n",
    "#  'Nov11_2019': 'Comments1',\n",
    "#  'Jul17_2020': 'Comments0',\n",
    "#  'Oct29_2020': 'Comments7'}\n",
    "\n",
    "\n",
    "# if False:\n",
    "#     for i,key1 in enumerate(key_list):\n",
    "#             bk = better_key(key1)\n",
    "\n",
    "#             standalone_html_comparer = references_html(final_dict[bk], doc_names=[better_key(key1)], doc_type=\"PIT Regulations\")\n",
    "\n",
    "#             with open(f\"../demo/All_HTMLS/PIT_Regs/comments/{dte[better_key(key1)]}.html\",'w') as f:\n",
    "#                 f.write(standalone_html_comparer)\n",
    "\n",
    "#             if better_key(key1) == \"Sep17_2019\":\n",
    "#                 print(dte[bk])\n",
    "#                 copyfile(\"../demo/safecopy.html\", f\"../demo/All_HTMLS/PIT_Regs/comments/{dte[better_key(key1)]}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ea894a791c8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mele\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<mark>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'</mark>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mregspecmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"regulation [1-9][0-9]*(\\([1-9][0-9]*\\)|[a-z]{0,2})\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_' is not defined"
     ]
    }
   ],
   "source": [
    "\"REGULATIONS MATCH\"\n",
    "\n",
    "import re\n",
    "c = 0\n",
    "for ele in list_:\n",
    "    cnt = ele[1].lower().replace('<mark>','').replace('</mark>','')\n",
    "    regspecmat = [ele.group() for ele in  re.finditer(r\"regulation [1-9][0-9]*(\\([1-9][0-9]*\\)|[a-z]{0,2})\", cnt)]\n",
    "    if len(regspecmat) != 0:\n",
    "        print(regspecmat)\n",
    "        print(cnt)\n",
    "        print(\"-----------------------------\")\n",
    "        c+=1\n",
    "              \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename_to_refs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e8308d2a5fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mall_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_grep_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_matches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mkey_for_glossary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmatchdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename_to_refs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_for_glossary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey_for_glossary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_matches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatchdct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename_to_refs' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "for reg_file in glob('../document_scraping/all_amend_rational.match.txt'):\n",
    "    with open(reg_file,'r') as f:\n",
    "        all_matches = f.read().split('--')\n",
    "        all_matches = [clean_grep_out(ele) for ele in all_matches]\n",
    "        key_for_glossary = os.path.basename(folder_name.replace('_',' '))\n",
    "        matchdct = filename_to_refs.get(key_for_glossary, [key_for_glossary.lower()]) \n",
    "        for filen, line in all_matches:    \n",
    "            if any([ele in line for ele in matchdct]):\n",
    "                line = detect_pattern(line,matchdct)\n",
    "                list_.append((filen, line, reg_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../document_scraping/all_amend_rational.match.txt') as f:\n",
    "    all_matches = f.read().split('\\n')\n",
    "    a = []\n",
    "    for ele in all_matches:\n",
    "        a.append(ele.split('-')[0])\n",
    "        a.append(ele.split(':')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR LATEST HTMLS\n",
    "\n",
    "# list_ = []\n",
    "# import os\n",
    "# for reg_file in glob('../document_scraping/*.txt'):\n",
    "#     with open(reg_file,'r') as f:\n",
    "#         all_matches = f.read().split('--')\n",
    "#         all_matches = [clean_grep_out(ele) for ele in all_matches]\n",
    "#         for file, line in all_matches:\n",
    "#             matchdct = [\"Prohibition of  Insider  Trading\", \"PIT\",\"Insider Trading\",\"insider trading\"]\n",
    "#             if any([ele in line for ele in matchdct]):\n",
    "#                 line = detect_pattern(line,matchdct)\n",
    "#                 list_.append((file, line, reg_file))\n",
    "\n",
    "# print(len(list_))\n",
    "# from datetime import datetime \n",
    "\n",
    "# count  = 0\n",
    "# for ele in list_:\n",
    "#     if ele not in all_files_dict:\n",
    "#         count += 1\n",
    "# print(count, len(list_))        \n",
    "\n",
    "# pitsl = ['Jan28_2021']\n",
    "# # , 'Aug16_2011', 'Jan15_2015', 'Dec31_2018', 'Jan21_2019', 'Sep17_2019', 'Nov11_2019', 'Jul17_2020', 'Oct29_2020'] \n",
    "\n",
    "# final_dict = {}\n",
    "# # setlis = set(list_)\n",
    "# set_indices = set()\n",
    "\n",
    "# while len(pitsl) != 0:\n",
    "#     key_date = pitsl.pop(0)\n",
    "#     dt1 = datetime.strptime(key_date, '%b%d_%Y')\n",
    "#     print(key_date,dt1)    \n",
    "#     final_dict[key_date] = []\n",
    "    \n",
    "    \n",
    "#     for i,ele in enumerate(list_):\n",
    "#         if i in set_indices:\n",
    "#             continue\n",
    "        \n",
    "#         # File not found\n",
    "#         if ele[0] not in all_files_dict:\n",
    "#             set_indices.add(i)\n",
    "#             continue\n",
    "        \n",
    "#         tim = all_files_dict[ele[0]][\"time\"] \n",
    "#         dt2 = datetime.strptime(tim, '%b %d, %Y')\n",
    "#         if dt2 <= dt1:\n",
    "#             cpd_dict = all_files_dict[ele[0]]\n",
    "#             cpd_dict[\"content\"] = ele[1]\n",
    "#             cpd_dict[\"dtm\"] = dt2\n",
    "#             final_dict[key_date].append(cpd_dict)\n",
    "#             set_indices.add(i)\n",
    "            \n",
    "# key_list = ['Jan28_2021']\n",
    "\n",
    "# dte = {'Jan28_2021': 'Comments0'}\n",
    "\n",
    "\n",
    "# if True:\n",
    "#     for i,key1 in enumerate(key_list):\n",
    "#             bk = better_key(key1)\n",
    "\n",
    "#             standalone_html_comparer = references_html(final_dict[bk], doc_names=[better_key(key1)], doc_type=\"PIT Regulations\")\n",
    "#             print(f\"../../data/FOR_SENDING/{dte[better_key(key1)]}.html\")\n",
    "#             with open(f\"../../data/FOR_SENDING/{dte[better_key(key1)]}.html\",'w') as f:\n",
    "#                 f.write(standalone_html_comparer)\n",
    "\n",
    "# al = final_dict['Jan28_2021']\n",
    "# al.sort(key=lambda x: x[\"dtm\"])\n",
    "\n",
    "# # from custom_functions import write_file\n",
    "# stand_html = references_html(list_, doc_names=[\"Jan30_2021\"], doc_type=\"PIT Regulations\")\n",
    "# with open('../../data/FOR_SENDING/all_pit_refs.html','w') as f:\n",
    "#     f.write(stand_html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}